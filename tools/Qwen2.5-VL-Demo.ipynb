{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T15:53:02.077469Z",
     "start_time": "2025-06-21T15:52:59.734238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n"
   ],
   "id": "cc6a42c8ccb60116",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01manomaly_qwen2_5_vl\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Qwen2_5_VLForConditionalGeneration, AutoProcessor\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mqwen_vl_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m process_vision_info\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# default: Load the model on the available device(s)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Research\\TGADV\\transformers\\utils\\import_utils.py:2045\u001B[39m, in \u001B[36m_LazyModule.__getattr__\u001B[39m\u001B[34m(self, name)\u001B[39m\n\u001B[32m   2043\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._class_to_module.keys():\n\u001B[32m   2044\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2045\u001B[39m         module = \u001B[38;5;28mself\u001B[39m._get_module(\u001B[38;5;28mself\u001B[39m._class_to_module[name])\n\u001B[32m   2046\u001B[39m         value = \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[32m   2047\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m, \u001B[38;5;167;01mRuntimeError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Research\\TGADV\\transformers\\utils\\import_utils.py:2075\u001B[39m, in \u001B[36m_LazyModule._get_module\u001B[39m\u001B[34m(self, module_name)\u001B[39m\n\u001B[32m   2073\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib.import_module(\u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m + module_name, \u001B[38;5;28mself\u001B[39m.\u001B[34m__name__\u001B[39m)\n\u001B[32m   2074\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m-> \u001B[39m\u001B[32m2075\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Research\\TGADV\\transformers\\utils\\import_utils.py:2073\u001B[39m, in \u001B[36m_LazyModule._get_module\u001B[39m\u001B[34m(self, module_name)\u001B[39m\n\u001B[32m   2071\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_get_module\u001B[39m(\u001B[38;5;28mself\u001B[39m, module_name: \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m   2072\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2073\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m importlib.import_module(\u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m + module_name, \u001B[38;5;28mself\u001B[39m.\u001B[34m__name__\u001B[39m)\n\u001B[32m   2074\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   2075\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\TGADV\\Lib\\importlib\\__init__.py:90\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m     88\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     89\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap._gcd_import(name[level:], package, level)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Research\\TGADV\\anomaly_qwen2_5_vl\\modeling_anomaly_qwen2_5_vl.py:35\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnn\u001B[39;00m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfunctional\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mF\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mactivations\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ACT2FN\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcache_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Cache, DynamicCache, SlidingWindowCache, StaticCache\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgeneration\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GenerationMixin\n",
      "\u001B[31mImportError\u001B[39m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-21T15:45:35.009184Z",
     "start_time": "2025-06-21T15:45:29.607129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Messages containing a local video path and a text query\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"./dataset/train/01_0029-180_203.mp4\",\n",
    "                \"fps\": 24.0,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"这段校园步行道视频里有异常情况吗？\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    **video_kwargs,\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n",
    "print(video_inputs[0].shape)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-vl-utils using decord to read video.\n",
      "Unused or unrecognized kwargs: fps, return_tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['从提供的图片来看，这段校园步行道上没有明显的异常情况。视频中可以看到一个骑自行车的人和一些行人，他们都沿着步行道正常行走或骑行。环境看起来很平静，周围有树木和绿化带，整体氛围比较和谐。不过，由于视频只提供了几张静态图片，无法判断是否有异常情况发生。如果有更多连续的视频帧或者音频信息，可能会更容易发现异常情况。']\n",
      "torch.Size([24, 3, 476, 868])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T14:48:33.308073Z",
     "start_time": "2025-06-21T14:48:33.306079Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8550ba670020d7d5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
